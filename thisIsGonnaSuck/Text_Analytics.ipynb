{"cells":[{"cell_type":"markdown","metadata":{"id":"oHEgjpPfROOJ"},"source":["### READ ME\n","\n","Use the code blocks below to answer each question. Only print the output required for each question. Do not edit the comments at the top of each code cell. Otherwise, the auto-grader may misinterpret your results. See Question 0 as an an example of how to complete a task (leave it in your notebook; don't delete it):"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1527,"status":"ok","timestamp":1711435145095,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":360},"id":"Le-kBTguRIGI","outputId":"260d40fc-70ae-4660-f538-cce778b0ff20"},"outputs":[],"source":["try:\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","except:\n","  pass"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":199},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1711435145095,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":360},"id":"rtSZVc6QRRsw","outputId":"79e52d80-0ec3-40d8-e933-060db8c5655f"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Col1</th>\n","      <th>Col2</th>\n","      <th>Col3</th>\n","      <th>Col4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Row1</th>\n","      <td>Row1/Col1</td>\n","      <td>Row1/Col2</td>\n","      <td>Row1/Col3</td>\n","      <td>Row1/Col4</td>\n","    </tr>\n","    <tr>\n","      <th>Row2</th>\n","      <td>Row2/Col1</td>\n","      <td>Row2/Col2</td>\n","      <td>Row2/Col3</td>\n","      <td>Row2/Col4</td>\n","    </tr>\n","    <tr>\n","      <th>Row3</th>\n","      <td>Row3/Col1</td>\n","      <td>Row3/Col2</td>\n","      <td>Row3/Col3</td>\n","      <td>Row3/Col4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           Col1       Col2       Col3       Col4\n","Row1  Row1/Col1  Row1/Col2  Row1/Col3  Row1/Col4\n","Row2  Row2/Col1  Row2/Col2  Row2/Col3  Row2/Col4\n","Row3  Row3/Col1  Row3/Col2  Row3/Col3  Row3/Col4"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["# Question 0: Create a DataFrame with three rows and four columns. Name the\n","# columns 'Col1', 'Col2', 'Col3', 'Col4'. Create an index for the DataFrame\n","# and give the rows the index values of 'Row1', 'Row2', 'Row3'. Place a value\n","# in each column equal to the {ColumnName/RowName}. e.g. Col1/Row1. Print\n","# the entire DataFrame.\n","\n","import pandas as pd\n","\n","df = pd.DataFrame(columns=['Col1', 'Col2', 'Col3', 'Col4'], index=['Row1', 'Row2', 'Row3'])\n","\n","for col in df:\n","  for i, value in df[col].items():\n","    df.at[i, col] = f'{i}/{col}'\n","\n","df"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":627,"status":"ok","timestamp":1711435145719,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":360},"id":"MkWvLRl3dHVC","outputId":"58641b91-8e0b-4f42-a851-dbb4742349b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Token: I, Lemma: I\n","Token: am, Lemma: be\n","Token: a, Lemma: a\n","Token: master, Lemma: master\n","Token: builder, Lemma: builder\n","Token: of, Lemma: of\n","Token: text, Lemma: text\n","Token: analytics, Lemma: analytic\n","Token: !, Lemma: !\n"]}],"source":["# Question 1:\n","# Import the spacy package (and install if if needed)\n","# Load the \"en_core_web_sm\" model into a variable\n","# Load the following text into that model: \"I am a master builder of text analytics!\"\n","# Print each token and the lemmatized version one-at-a-time using a loop\n","\n","import spacy\n","\n","nlp = spacy.load('en_core_web_sm')\n","text = \"I am a master builder of text analytics!\"\n","doc = nlp(text)\n","\n","for token in doc:\n","    print(f\"Token: {token.text}, Lemma: {token.lemma_}\")\n"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1711435145719,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":360},"id":"NqpnaiXRdHVC","outputId":"c58ac2d7-ca29-4a5f-945d-bc3a24d70682"},"outputs":[],"source":["# Question 2:\n","# Conceptual question; no code required here."]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":387},"executionInfo":{"elapsed":956,"status":"ok","timestamp":1711435146674,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":360},"id":"wzvaRPKIdHVD","outputId":"3930a496-d791-40d5-bbc6-a4d74d87a317"},"outputs":[{"ename":"KeyError","evalue":"'isStop'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n","File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n","File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n","\u001b[1;31mKeyError\u001b[0m: 'isStop'","\nThe above exception was the direct cause of the following exception:\n","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[1;32mIn[29], line 33\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     31\u001b[0m create_spacy_df(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI am a master builder of text analytics!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m non_stop_words_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df[\u001b[38;5;241m~\u001b[39m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43misStop\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m])\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of tokens after removing stop words:\u001b[39m\u001b[38;5;124m\"\u001b[39m, non_stop_words_count)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n","\u001b[1;31mKeyError\u001b[0m: 'isStop'"]}],"source":["# Question 3:\n","# Create a function that takes a string as input and builds a DataFrame based on the\n","# properties returned by the spacy model. The DataFrame should have the following columns:\n","# Index: Token, Columns: Lemma, POS, Tag, Dep, Shape, isAlpha, isStop\n","# Call the function using the same string as in Question 1 and print the resulting DataFrame.\n","\n","\n","nlp = spacy.load('en_core_web_sm')\n","\n","def create_spacy_df(text):\n","    doc = nlp(text)\n","\n","    data = []\n","    for token in doc:\n","        toke_data = {\n","            'Token': token.text,\n","            'Lemma': token.lemma_,\n","            'POS': token.pos_,\n","            'Tag': token.tag_,\n","            'Dep': token.dep_,\n","            'Shape': token.shape_,\n","            'isAlpha': token.is_alpha,\n","            'isStop': token.is_stop\n","        }\n","        data.append(toke_data)\n","\n","        df = pd.DataFrame(data)\n","        df.set_index('Token', inplace=True)\n","        return df\n","    \n","create_spacy_df(\"I am a master builder of text analytics!\")\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1711435146674,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":360},"id":"xYKffKU4dHVD","outputId":"156976ad-092a-4ef3-de62-d5da480ec9cb"},"outputs":[],"source":["# Question 4:\n","# Conceptual question; no code required here."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":831,"status":"ok","timestamp":1711435147502,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":360},"id":"iBj8_Sw-dHVD","outputId":"6f1e21d6-772d-4434-e33a-e9d27f43e2a1"},"outputs":[],"source":["# Question 5:\n","# Create a function that takes a string as input and returns only the lemmatized\n","# tokens that are not stop words. Also, create a parameter for the function that\n","# allows the user to specify which parts of speech should be kept. Return the string\n","# as a list of words.\n","\n","# Call the function using the same string as in the first question. Only allow nouns,\n","# verbs, adjectives, and adverbs to be returned. Print the resulting list without modifications.\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1711435147502,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":360},"id":"dcNTlfQwdHVE","outputId":"cbe0cdb8-ce56-4c19-f75d-0536cda910f5"},"outputs":[],"source":["# Question 6:\n","# Conceptual question; no code required here."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":535},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1711435147503,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":360},"id":"IvltDJj6SKVW","outputId":"071c19e2-a403-4741-a65f-6645f8a075ce"},"outputs":[],"source":["# Question 7: Import the dataset that came with this assignment into a Pandas DataFrame.\n","# Print the shape and the first five rows of the DataFrame.\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":716},"executionInfo":{"elapsed":10076,"status":"ok","timestamp":1711435157574,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":360},"id":"RUFOYJJ-dHVE","outputId":"56968224-05af-4ee3-d0d9-c20bff838c44"},"outputs":[],"source":["# Question 8:\n","# Using either the functions provided in the book chapter or some that you create yourself,\n","# add the following columns to this DataFrame:\n","\n","# - Number of words or tokens in the post\n","# - Number of Nouns in the post\n","# - Number of Verbs in the post\n","# - Number of Adjectives in the post\n","# - Number of Adverbs in the post\n","# - Number of Persons referenced in the post\n","# - Number of Organizations referenced in the post\n","# - Number of Locations (GPE) referenced in the post\n","# - Number of Products referenced in the post\n","# - Number of Events referenced in the post\n","\n","# These counts should all be based on the determinations made by the spacy model. Print the\n","# first 5 records in the DataFrame with these new columns. Also print the sum of each of these\n","# new columns.\n","\n","# How many total events are referenced in the dataset based on the spacy model?\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1711435157574,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":360},"id":"XKH0Gc06Y6lz","outputId":"269298fa-1a15-4752-e466-9631c9500e48"},"outputs":[],"source":["# Question 9:\n","# Next, we are going to build a topic model for this dataset. As usual, follow the instructions in the order they are given to be successful. The process flows like a pipeline, unless specified otherwise, you will use the data generated in each step for the step that follows in a linear fashion--just like it flows in the chapter. You are welcome to use any of the functions you find in the chapter. But, although it is good practice to create a pipeline with functions, using functions is not required for the rest of this assignment.\n","\n","# First, create a dataset of the post 'text' column that has all email addresses, new line breaks, single quotes, and urls removed. You can store this cleaned dataset however you'd like: in a list, a Pandas Series, as a new column in the original DataFrame, or something else. We are following the process that we went through in the chapter. If you're not sure what I'm talking about, then you probably need to take a look at the chapter.\n","\n","# Print out the first five documents in the dataset after being cleaned as described above. Remember, this is the list of text documents (posts). Not the entire DataFrame.\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8287,"status":"ok","timestamp":1711435165847,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":360},"id":"e-PpJ4TTZ0T8","outputId":"0a7aa722-6d94-4b18-cc26-d2a26cb7addb"},"outputs":[],"source":["# Question 10:\n","# Next, we are going to tokenize the documents. At the same time, we will remove stop words and punctuation. We will also return the lemmatized version of each token--just like we did in the chapter. Perform that step on the dataset you created in the previous question. Print out the first five documents in the dataset after being tokenized.\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1711435165847,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":360},"id":"hmzD8HyrcKw_","outputId":"71aef429-fa2d-427f-f555-75bec604df32"},"outputs":[],"source":["# Question 11: Build bigram, trigram, and fourgram models. Use a threshold of 50 and\n","# a min_count of 5 for each of them. Apply these models to the dataset you created in\n","# the previous question. Print out the first five documents in the dataset after being\n","# processed by the bigram, trigram, and fourgram models.\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2049,"status":"ok","timestamp":1711435167886,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":360},"id":"rbK6I3A3c0zs","outputId":"ee7a4450-7dfc-4fce-b252-b58897d747c2"},"outputs":[],"source":["# Question 12: Create a dictionary and courpus and build your LDA\n","# model using 4 topics. Set the random_state to 1. Set the\n","# chuncksize to 20, passes to 10, and per_word_topics to True. Do not\n","# set or adjust any other parameters (even if the example in the\n","# book does). Print out the topic weights for the 10 most important\n","# words in each topic.\n","\n","# What weight does the term \"autism\" have on Topic 0?\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"elapsed":18979,"status":"ok","timestamp":1711435186863,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":360},"id":"5uCtwz8sCNtB","outputId":"385199bc-96a3-4ee8-9416-b9970a8e026a"},"outputs":[],"source":["# Question 13: Generate LDA models for n = 3 through 9 topics and compare their perplexity\n","# and coherence scores. Keep all other parameter settings used in the prior question.\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":940},"executionInfo":{"elapsed":896,"status":"ok","timestamp":1711435187756,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":360},"id":"Q__dOXw3CbhY","outputId":"366a71be-6248-4c29-a8fe-cae4ea59a479"},"outputs":[],"source":["# Question 14: Visualize results of comparing the perplexity/coherence from the previous\n","# question.\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":843},"executionInfo":{"elapsed":2157,"status":"ok","timestamp":1711435189910,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":360},"id":"u3CtjbhygVhr","outputId":"dfdda049-b2d7-4092-abea-f8908ef40926"},"outputs":[],"source":["# Question 15: Build one more LDA model with 9 topics. Keep the other parameters the same as all prior LDA models. Then, generate new features (one for each topic) and then generate a topic score for every document. Add these new topic scores to the version of the DataFrame that includes the new features representing the counts of parts of speech and named entities required earlier in this assignment.\n"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":5773,"status":"ok","timestamp":1711435195680,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":360},"id":"DX_CvHFai9JZ","outputId":"27db1476-fa7a-407f-e312-f9df7afbbe42"},"outputs":[],"source":["# Question 16:\n","# Create a bar chart of word counts for all topics. Plot Word Count and Weights\n","# of Topic Keywords as demonstrated in the chapter.\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":800},"executionInfo":{"elapsed":995,"status":"ok","timestamp":1711435196665,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":360},"id":"Sj98oGIUmklT","outputId":"7a1c09d3-79c9-475e-da1f-8ff91e114e64"},"outputs":[],"source":["# Question 17:\n","# Visualize the number of documents/tweets attributed to each topic is through\n","# a t-distributed Stochastic Neighbor Embedding (SNE) chart.\n","\n"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":917},"executionInfo":{"elapsed":2873,"status":"ok","timestamp":1711435199536,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":360},"id":"yApDHh8lmrBf","outputId":"5e9ff6bb-03f1-4480-8636-b54c641088ec"},"outputs":[],"source":["# Question 18:\n","# Print out an interactive visualization with pyLDAvis.\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
