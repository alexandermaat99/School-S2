{"cells":[{"cell_type":"markdown","metadata":{"id":"xuIi7GMxOv2T"},"source":["### READ ME\n","\n","Use the code blocks below to answer each quiz question. Only print the output required for each question. Do not edit the comments at the top of each code cell. Otherwise, the auto-grader may misinterpret your results. See Question 0 as an an example of how to complete a task (leave it in your notebook; don't delete it):"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"elapsed":139,"status":"ok","timestamp":1709753327830,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":420},"id":"WFavhnibOxsg","outputId":"6e6942b1-ae18-426a-a681-0d01f9585d99"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Col1</th>\n","      <th>Col2</th>\n","      <th>Col3</th>\n","      <th>Col4</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Row1</th>\n","      <td>Row1/Col1</td>\n","      <td>Row1/Col2</td>\n","      <td>Row1/Col3</td>\n","      <td>Row1/Col4</td>\n","    </tr>\n","    <tr>\n","      <th>Row2</th>\n","      <td>Row2/Col1</td>\n","      <td>Row2/Col2</td>\n","      <td>Row2/Col3</td>\n","      <td>Row2/Col4</td>\n","    </tr>\n","    <tr>\n","      <th>Row3</th>\n","      <td>Row3/Col1</td>\n","      <td>Row3/Col2</td>\n","      <td>Row3/Col3</td>\n","      <td>Row3/Col4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           Col1       Col2       Col3       Col4\n","Row1  Row1/Col1  Row1/Col2  Row1/Col3  Row1/Col4\n","Row2  Row2/Col1  Row2/Col2  Row2/Col3  Row2/Col4\n","Row3  Row3/Col1  Row3/Col2  Row3/Col3  Row3/Col4"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Question 0: Create a DataFrame with three rows and four columns. Name the\n","# columns 'Col1', 'Col2', 'Col3', 'Col4'. Create an index for the DataFrame\n","# and give the rows the index values of 'Row1', 'Row2', 'Row3'. Place a value\n","# in each column equal to the {ColumnName/RowName}. e.g. Col1/Row1. Print\n","# the entire DataFrame.\n","\n","import pandas as pd\n","\n","df = pd.DataFrame(columns=['Col1', 'Col2', 'Col3', 'Col4'], index=['Row1', 'Row2', 'Row3'])\n","\n","for col in df:\n","  for i, value in df[col].items():\n","    df.at[i, col] = f'{i}/{col}'\n","\n","df"]},{"cell_type":"markdown","metadata":{"id":"WU7-9V9FOSJy"},"source":["# Import Data"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":141,"status":"ok","timestamp":1709759810568,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":420},"id":"AQNFLwYQNkcJ"},"outputs":[],"source":["# Question 1: Create a function to import the data that allows a user to enter a path to a csv file\n","# and include a parameter that allows them to decide whether they want to display the number\n","# of rows and columns in the dataset. Also, allow the user to randomly sample the dataset\n","# size down to an inputted number of rows that is defaulted to 500. Always use a random seed = 1\n","def importData (data, sample_size=500, show_shape=True):\n","  df = pd.read_csv(data)\n","  if show_shape:\n","    print(f'The dataset has {df.shape[0]} rows and {df.shape[1]} columns')\n","  df = df.sample(n=sample_size, random_state=1)\n","  return df\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":585},"executionInfo":{"elapsed":399,"status":"ok","timestamp":1709759811127,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":420},"id":"LVylLTAwN4RE","outputId":"f30e2bcc-142e-4b2c-b52e-5db80b97ac90"},"outputs":[{"name":"stdout","output_type":"stream","text":["The dataset has 24037 rows and 25 columns\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>context_annotations_count</th>\n","      <th>count_annotations</th>\n","      <th>count_cashtags</th>\n","      <th>count_hashtags</th>\n","      <th>count_mentions</th>\n","      <th>count_urls</th>\n","      <th>created_at_tweet</th>\n","      <th>lang</th>\n","      <th>likes</th>\n","      <th>...</th>\n","      <th>source</th>\n","      <th>author_followers_count</th>\n","      <th>author_following_count</th>\n","      <th>author_tweet_count</th>\n","      <th>author_listed_count</th>\n","      <th>author_verified</th>\n","      <th>media_type</th>\n","      <th>height</th>\n","      <th>width</th>\n","      <th>preview_image_url</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>84</th>\n","      <td>Curious about the Coronavirus? Where it came f...</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2020-01-31T15:45:10.000Z</td>\n","      <td>en</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>The Social Jukebox</td>\n","      <td>85885</td>\n","      <td>70720</td>\n","      <td>54</td>\n","      <td>955</td>\n","      <td>False</td>\n","      <td>photo</td>\n","      <td>606</td>\n","      <td>1007</td>\n","      <td>https://pbs.twimg.com/media/EPnugCjX4AIn6tK.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>5220</th>\n","      <td>Remember to take it one day at the time! üíê üå∑ üåπ...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2020-04-24T20:13:02.000Z</td>\n","      <td>en</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>SocialBee.io v2</td>\n","      <td>241</td>\n","      <td>563</td>\n","      <td>98</td>\n","      <td>7</td>\n","      <td>False</td>\n","      <td>photo</td>\n","      <td>1080</td>\n","      <td>1080</td>\n","      <td>https://pbs.twimg.com/media/EWZRWklX0AI4Xv3.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>22827</th>\n","      <td>Death by Indifference... health inequalities s...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2021-07-19T13:04:16.000Z</td>\n","      <td>en</td>\n","      <td>3</td>\n","      <td>...</td>\n","      <td>Twitter Web App</td>\n","      <td>1937</td>\n","      <td>4985</td>\n","      <td>95</td>\n","      <td>34</td>\n","      <td>False</td>\n","      <td>photo</td>\n","      <td>2304</td>\n","      <td>4096</td>\n","      <td>https://pbs.twimg.com/media/E6qT_H5WYAgnGc_.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>5794</th>\n","      <td>I‚Äôve been taking treats to my Autistic kiddos ...</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2020-04-29T23:26:07.000Z</td>\n","      <td>en</td>\n","      <td>50</td>\n","      <td>...</td>\n","      <td>Twitter for iPhone</td>\n","      <td>2519</td>\n","      <td>2564</td>\n","      <td>95</td>\n","      <td>38</td>\n","      <td>False</td>\n","      <td>photo</td>\n","      <td>2048</td>\n","      <td>1538</td>\n","      <td>https://pbs.twimg.com/media/EWztfW_U8AEwuj4.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>4179</th>\n","      <td>Children and adults living with autism may fin...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2020-04-15T16:15:13.000Z</td>\n","      <td>en</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>Hootsuite Inc.</td>\n","      <td>568</td>\n","      <td>552</td>\n","      <td>100</td>\n","      <td>15</td>\n","      <td>False</td>\n","      <td>photo</td>\n","      <td>630</td>\n","      <td>1200</td>\n","      <td>https://pbs.twimg.com/media/EVqEndtXkAEzcQ_.jpg</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows √ó 25 columns</p>\n","</div>"],"text/plain":["                                                    text  \\\n","84     Curious about the Coronavirus? Where it came f...   \n","5220   Remember to take it one day at the time! üíê üå∑ üåπ...   \n","22827  Death by Indifference... health inequalities s...   \n","5794   I‚Äôve been taking treats to my Autistic kiddos ...   \n","4179   Children and adults living with autism may fin...   \n","\n","       context_annotations_count  count_annotations  count_cashtags  \\\n","84                             6                  0               0   \n","5220                           1                  0               0   \n","22827                          1                  1               0   \n","5794                           3                  0               0   \n","4179                           1                  0               0   \n","\n","       count_hashtags  count_mentions  count_urls          created_at_tweet  \\\n","84                  0               0           2  2020-01-31T15:45:10.000Z   \n","5220                5               0           1  2020-04-24T20:13:02.000Z   \n","22827               1               0           1  2021-07-19T13:04:16.000Z   \n","5794                1               0           1  2020-04-29T23:26:07.000Z   \n","4179                2               0           2  2020-04-15T16:15:13.000Z   \n","\n","      lang  likes  ...              source  author_followers_count  \\\n","84      en      0  ...  The Social Jukebox                   85885   \n","5220    en      1  ...     SocialBee.io v2                     241   \n","22827   en      3  ...     Twitter Web App                    1937   \n","5794    en     50  ...  Twitter for iPhone                    2519   \n","4179    en      1  ...      Hootsuite Inc.                     568   \n","\n","       author_following_count author_tweet_count  author_listed_count  \\\n","84                      70720                 54                  955   \n","5220                      563                 98                    7   \n","22827                    4985                 95                   34   \n","5794                     2564                 95                   38   \n","4179                      552                100                   15   \n","\n","      author_verified  media_type  height  width  \\\n","84              False       photo     606   1007   \n","5220            False       photo    1080   1080   \n","22827           False       photo    2304   4096   \n","5794            False       photo    2048   1538   \n","4179            False       photo     630   1200   \n","\n","                                     preview_image_url  \n","84     https://pbs.twimg.com/media/EPnugCjX4AIn6tK.jpg  \n","5220   https://pbs.twimg.com/media/EWZRWklX0AI4Xv3.jpg  \n","22827  https://pbs.twimg.com/media/E6qT_H5WYAgnGc_.jpg  \n","5794   https://pbs.twimg.com/media/EWztfW_U8AEwuj4.jpg  \n","4179   https://pbs.twimg.com/media/EVqEndtXkAEzcQ_.jpg  \n","\n","[5 rows x 25 columns]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# Question 2: Call that function using the dataset provided with this assignment. Sub-sample the dataset down\n","# to 500 rows using the parameter provided in the function you created. Specify the parameter\n","# needed to allow the function to print the shape of the DataFrame\n","\n","# Print the last five rows of the DataFrame\n","df = importData('tw_tweets_users_media.csv', sample_size=500, show_shape=True)\n","df.tail()"]},{"cell_type":"markdown","metadata":{"id":"07YxsRq-OWUO"},"source":["# Explore Data"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1709759811566,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":420},"id":"upOI9KtGOB64"},"outputs":[],"source":["# Question 3: Create a function to calculate the following univariate properties: data type, number of missing values,\n","# number of unique values, min, median, max, mode, mean, standard deviation, and skewness. These metrics\n","# should be summarized in a table as demonstrated in the chapter. However, do not include any measures other\n","# than those listed here. Create the function so that the appropriate metrics will be only calculated for the\n","# correct data types: categorical or numeric as demonstrated in the chapter example.\n","\n","# Also print out a countplot for every categorical or binary (only two values) feature. To keep this simple,\n","# do now calculate histograms for each numeric feature (although that would normally be a good idea). You can\n","# choose the exact display formats for both the univariate properties and visualizations. There is an example\n","# in the chapter of this type of function. If you copy that example, you will need to remove the histogram\n","# loop. In addition, you'll need to remove any metrics calculated in the function from the book that aren't\n","# required for this question.\n","\n","def univariate(df, sample=500):\n","    import seaborn as sns \n","    import matplotlib.pyplot as plt\n","    import math \n","\n","    dfResults = pd.DataFrame(columns=['bin_groups', 'type', 'missing', 'unique', 'min', 'median', 'max', 'mode', 'mean', 'std', 'skewness'])\n","\n","\n","    for col in df:\n","      dtype = df[col].dtype\n","      missing = df[col].isna().sum()\n","      unique = df[col].nunique()\n","      mode = df[col].mode()[0]\n","      if pd.api.types.is_numeric_dtype(df[col]):\n","        min = df[col].min()\n","        max = df[col].max()\n","        mean = df[col].mean()\n","        median = df[col].median()\n","        std = df[col].std()\n","        skew = df[col].skew()\n","        dfResults.loc[col] = ['-', dtype, missing, unique, min, median, max, mode,\n","                              round(mean, 2), round(std, 2), round(skew, 2)]\n","      else:\n","        flag = df[col].value_counts()[(df[col].value_counts() / df.shape[0]) < 0.05].shape[0]\n","        dfResults.loc[col] = [flag, dtype, missing, unique, '-', '-', '-', mode, '-', '-', '-']\n","\n","    countplots = dfResults[(dfResults['type']=='object') | (dfResults['unique']==2)]\n","    histograms = dfResults[(dfResults['type']=='float64') | ((dfResults['unique']>10) & (dfResults['type']=='int64'))]\n","    histograms = histograms[histograms['unique']>2]\n","\n","    f, ax = plt.subplots(1, countplots.shape[0], figsize=[countplots.shape[0] * 1.5, 1.5])\n","    for i, col in enumerate(countplots.index):\n","      g = sns.countplot(data=df, x=col, color='g', ax=ax[i]);\n","      g.set_yticklabels('')\n","      g.set_ylabel('')\n","      ax[i].tick_params(labelrotation=90, left=False)\n","      ax[i].xaxis.set_label_position('top')\n","      sns.despine(left=True, top=True, right=True)\n","\n","    plt.subplots_adjust(hspace=2, wspace=.5)\n","    plt.show()\n","\n","    f, ax = plt.subplots(1, histograms.shape[0], figsize=[histograms.shape[0] * 1.5, 1.5])\n","    for i, col in enumerate(histograms.index):\n","        g = sns.histplot(data=df.sample(n=sample, random_state=1), x=col, color='b', ax=ax[i], kde=True);\n","        g.set_yticklabels(labels=[])\n","        g.set_ylabel('')\n","        ax[i].tick_params(left=False)\n","        sns.despine(left=True, top=True, right=True)\n","\n","    plt.subplots_adjust(hspace=2, wspace=.5)\n","    plt.show()\n","\n","    return dfResults\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":23659,"status":"ok","timestamp":1709759836922,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":420},"id":"yvpjhZ1IOYS2","outputId":"d475655b-c905-425a-9123-7a16a8e0ad17"},"outputs":[],"source":["# Question 4: Call the function you just created to generate univariate properties and visualizations using the 500 row sample of data you imported\n","univariate(df)"]},{"cell_type":"markdown","metadata":{"id":"aB5zkBsrZGjd"},"source":["# Data Preparation"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":518,"status":"ok","timestamp":1709764116255,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":420},"id":"lMJsj75JWvmF","outputId":"4ba7da00-cbb8-495d-9149-0e6dfd8df490"},"outputs":[{"name":"stdout","output_type":"stream","text":["text\n","context_annotations_count\n","count_annotations\n","count_cashtags\n","count_hashtags\n","count_mentions\n","count_urls\n","created_at_tweet\n","lang\n","likes\n","quotes\n","referenced_tweet_count\n","replies\n","reply_settings\n","retweets\n","source\n","author_followers_count\n","author_following_count\n","author_tweet_count\n","author_listed_count\n","author_verified\n","media_type\n","height\n","width\n","preview_image_url\n"]}],"source":["# Question 5: Based on the univariate analyses in the prior step, it appears that we need to drop a few features and bin some categorical\n","# values of some of the features. First, the features \"text\" and \"preview_image_url\" contain too many unique values to dummy\n","# code for modeling. Let's drop those. Also, notice from the countplot generated for the \"reply_settings\" feature that almost\n","# every record is set to \"everyone\". In other words, this feature will not be very reliable when it comes time to make\n","# predictions. Lastly, we have four options for labels that would indicate how 'viral' each post is: quotes, likes, replies,\n","# and retweets. To simplify the label, let's create a new label that is the sum of each of these outcomes and then drop each\n","# of those individual labels.\n","\n","# Create a function that will drop those three features specified, sum together the four label options into a new label called,\n","# 'reach', and then drop the four labels. This type of function will be used only for this dataset since it is so specific to\n","# these features. In other words, you do not need to give this function logic to make these decisions dynamically. Just make\n","# it manually perform those steps and return the updated DataFrame:\n","\n","# 1. Drop 'text', 'preview_image_url', 'reply_settings'\n","# 2. Generate 'reach' as the sum of 'likes', 'quotes', 'replies', and 'retweets'\n","# 3. Drop 'likes', 'quotes', 'replies', and 'retweets'\n","\n","# Run this function in the full pipeline (i.e. after importing the entire dataset) and then print out all remaining feature\n","# names one-at-a-time.\n","def process_dataset(df):\n"," \n","    df = df.drop(['text', 'preview_image_url', 'reply_settings'], axis=1)\n","\n","    df['reach'] = df['likes'] + df['quotes'] + df['replies'] + df['retweets']\n","\n","    df = df.drop(['likes', 'quotes', 'replies', 'retweets'], axis=1)\n","\n","    return df\n","\n","process_dataset(df)\n","\n","for col in df:\n","  print(col)"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":125,"status":"ok","timestamp":1709764116993,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":420},"id":"Fy1G0Bt6oPa0"},"outputs":[{"name":"stdout","output_type":"stream","text":["text has been binned by setting Index(['RT @Mumbomania: Boys &amp; Girls, let‚Äôs have a look at The Guardian in April regarding Autism and Lockdown rules. https://t.co/mzkfoMWYcp',\n","       'RT @njdotcom: N.J. balloon artist with autism creates pieces for essential workers https://t.co/TRghFlmATD https://t.co/gFOzQhNv3o',\n","       'Lockdown solves ONE PROBLEM WHAT ABOUT ALL OTHERS?? What about those who live in homes riddled with domestic violence? What about children who run to school to get away from the trauma/abuse? What about those with autism/learning difficulties? OPEN YOUR EYES! https://t.co/034iEDbEnI',\n","       'Dog helping autistic teen with lockdown https://t.co/EQ5Uq0pFoh https://t.co/EHwKEnlhx4',\n","       '#Autism #AutismAwareness #autismmom #pandemic #COVID19 #coronavirus #specialneeds #warriormom https://t.co/7MwAOAr31t',\n","       '@adilray I've been at home with my 5 autistic children/adults since feb 2020! Not gone out once! Everyone is selfish and ling covid ignored! Science shows covid can infect your cells in as little as 6 hours and antibodies dont work!! Nobody should be exposed to covid period! https://t.co/PPv61oN7Xw',\n","       'During the restrictions in place due to the COVID-19 pandemic, @sasi has a range of online activities on offer to keep us connected. Head to https://t.co/tnIBvcHs3p for more details and book through our online booking system by selecting Book a Service‚ú® #SASI #autism #asd https://t.co/KW9e0GziCo',\n","       'Two new sessions have been added to the 2020 ASCLS &amp; AGT Virtual Joint Annual Meeting educational program: The Vaccine-Autism Controversy w/ @BekahMartin and Scaling Up Samples in Response to COVID-19 #IamASCLS #TheRealAGT #Labucate20 https://t.co/mDYDUlJOtD https://t.co/ue61JTIByJ',\n","       '#Mentalhealth #learningdisabilities Changes in routine may create additional stress for #children or #youngpeople with an #autistic condition and or a #learningdisability. Useful advice for parents and carers can be found here: :https://t.co/9DJ892sAJJ #LondonTogether https://t.co/YQzJ3fbDcO',\n","       'Bethany has been making sure her Service Dog Gia gets her hair done during Lockdown üêæ Look at that smile üêæ #autismservicedog #servicedog #autism #asd #workingdog #greenjacketdog #mcc #mycaninecompanion https://t.co/FoCCwz5kCe',\n","       ...\n","       'Learning (ish) to touch type, hoping it will help. #autism #lockdown https://t.co/AwciP4hNFr',\n","       'How is it Telegraph ‚Äòjournalists‚Äô like this can tweet Covid misinformation without any comeback? You‚Äôd have thought she would have learnt from her role in promoting the false conspiracy that MMR caused autism. https://t.co/1aBSfdS2Ch',\n","       '#publichealth #beatNCDs #NCDs #SDGs #GreenRecovery #GreenNewDeal #COVID19 #datascience #datamapping #bioinformatics #metabolomics #publichealth #dementia #cancer #breastcancer #pregnancy #biossensors #innovation #healthpromotion #nutrition #indigenous #futurism #Python #autism https://t.co/VWpDoVQmqY',\n","       'How Parents Of Autism Can Deal With Stress And Online School During Coronavirus https://t.co/hxSVLMrzTE https://t.co/yKQiJB02df',\n","       'An interesting article we came across on https://t.co/goB3ixnCaM to help individuals and carers of Autism dependents cope during the uncertainty of the Coronavirus outbreak. Annakennedyonline #staysafe #socialenterprise #supportlocal https://t.co/vwRCtY2w7M',\n","       '@JohnStamos My 5th child has fought long and hard for this day. He has autism so graduating meant everything to him and to us. One thing COVID cannot take is the feeling of accomplishment he and his classmates feel. These kids are something special and the blessing has been ours. https://t.co/YN55rETGTK',\n","       'Lovely to welcome @kaplinskyn and @lynnanneperry to our @BridgendCBC Atebion service today, showing how moving online during Covid has helped us change the lives of more families touched by autism than ever before. https://t.co/WwFlvqFapq',\n","       'For some, a second lockdown means spending more time at home. Relationships might start to feel strained and you might find you or your family snapping at each other. Try these ways of #communicating with your #family to support healthy relationships. https://t.co/YW1Z1cpzyE https://t.co/Mh6sdhFzXH',\n","       '@fscarfe Agree nothing about us without ALL #disabilities regardless if it is nature or nurture. But we bet #FASD walks among #Autism because it is is campaigned as crisis &amp; has monopoly on funding. Kinda like #coronavirus when more people die of lack of #alcoholawareness and #starvation https://t.co/ihD1T4wZnF',\n","       'Children and adults living with autism may find dealing with #coronavirus challenging. Here are our 10 tips for managing autism in the time of COVID-19. https://t.co/BAFN12bz8B #autismawarenessmonth https://t.co/QIekP37Jm3'],\n","      dtype='object', name='text', length=459) to Other\n","created_at_tweet has been binned by setting Index(['2020-04-17T19:12:34.000Z', '2020-04-24T08:15:06.000Z',\n","       '2021-04-02T07:35:00.000Z', '2020-04-15T14:00:00.000Z',\n","       '2020-08-14T08:54:17.000Z', '2021-06-04T09:57:07.000Z',\n","       '2020-11-30T00:01:03.000Z', '2020-05-20T20:35:04.000Z',\n","       '2020-05-21T09:14:31.000Z', '2020-05-04T09:05:23.000Z',\n","       ...\n","       '2020-08-11T21:45:14.000Z', '2020-07-26T17:12:21.000Z',\n","       '2020-04-02T10:59:05.000Z', '2020-05-23T06:06:47.000Z',\n","       '2021-05-24T15:58:58.000Z', '2020-11-29T09:00:12.000Z',\n","       '2020-03-07T23:59:03.000Z', '2020-08-02T16:50:05.000Z',\n","       '2020-04-30T23:18:08.000Z', '2020-04-15T16:15:13.000Z'],\n","      dtype='object', name='created_at_tweet', length=500) to Other\n","lang has been binned by setting Index(['und', 'tl', 'in', 'es', 'cy', 'fr', 'ro'], dtype='object', name='lang') to Other\n","reply_settings has been binned by setting Index(['following'], dtype='object', name='reply_settings') to Other\n","source has been binned by setting Index(['TweetDeck', 'dlvr.it', 'Twitter for iPad', 'IFTTT',\n","       'The Social Jukebox', 'Twitter Media Studio', 'Zoho Social',\n","       'Crowdfire App', 'CoSchedule', 'Sendible', 'EveryoneSocial',\n","       'SocialBee.io v2', 'Fanbooster by Traject', 'Grabyo', 'GWP Digital',\n","       'LaterMedia', 'Twitter for Advertisers.', 'Orlo', 'True Anthem',\n","       'Autism_twitter', 'Constant Contact - Social Posts', 'SocialFlow',\n","       'Echobox', 'ContentCal Studio', 'Publer.io', 'iweller.com',\n","       'Tweetbot for iŒüS', 'eClincher', 'Missinglettr', 'SocialNewsDesk'],\n","      dtype='object', name='source') to Other\n","preview_image_url has been binned by setting Index(['https://pbs.twimg.com/media/EY0gv2yXkAA-uPi.jpg',\n","       'https://pbs.twimg.com/media/EXBEiwbWkAQTQlp.jpg',\n","       'https://pbs.twimg.com/ext_tw_video_thumb/1251226264607895558/pu/img/e633PrBN2rC44odh.jpg',\n","       'https://pbs.twimg.com/media/EVlPejdWAAIwUJo.jpg',\n","       'https://pbs.twimg.com/media/EfXoEL2WAAE9McO.jpg',\n","       'https://pbs.twimg.com/media/E3B53F7XwAIgB-Z.jpg',\n","       'https://pbs.twimg.com/media/EoB5uGBXIAAePGt.jpg',\n","       'https://pbs.twimg.com/media/EYfPvGOXYAEyjCt.jpg',\n","       'https://pbs.twimg.com/media/EYh9j6DWAAAi2P3.jpg',\n","       'https://pbs.twimg.com/media/EXKYbm_WoAM0oQJ.jpg',\n","       ...\n","       'https://pbs.twimg.com/media/E48Wm-9VkAE7I5-.jpg',\n","       'https://pbs.twimg.com/media/E-g6S1jXMAENbZC.jpg',\n","       'https://pbs.twimg.com/media/EfK7qh1VAAEHL1x.png',\n","       'https://pbs.twimg.com/media/Ed3j2xQWoAEa2IS.jpg',\n","       'https://pbs.twimg.com/media/EUl_lGUX0AE8MIQ.jpg',\n","       'https://pbs.twimg.com/media/EYrlxRlUMAA5wem.jpg',\n","       'https://pbs.twimg.com/media/E2KjMQUXoAM2izl.jpg',\n","       'https://pbs.twimg.com/media/En-riK1XMAAxjTr.png',\n","       'https://pbs.twimg.com/tweet_video_thumb/ESi4xWOXgAAgN1Z.jpg',\n","       'https://pbs.twimg.com/media/EVqEndtXkAEzcQ_.jpg'],\n","      dtype='object', name='preview_image_url', length=459) to Other\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>context_annotations_count</th>\n","      <th>count_annotations</th>\n","      <th>count_cashtags</th>\n","      <th>count_hashtags</th>\n","      <th>count_mentions</th>\n","      <th>count_urls</th>\n","      <th>created_at_tweet</th>\n","      <th>lang</th>\n","      <th>likes</th>\n","      <th>...</th>\n","      <th>source</th>\n","      <th>author_followers_count</th>\n","      <th>author_following_count</th>\n","      <th>author_tweet_count</th>\n","      <th>author_listed_count</th>\n","      <th>author_verified</th>\n","      <th>media_type</th>\n","      <th>height</th>\n","      <th>width</th>\n","      <th>preview_image_url</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>4427</th>\n","      <td>Other</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>Other</td>\n","      <td>en</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>Twitter for iPhone</td>\n","      <td>1945</td>\n","      <td>1839</td>\n","      <td>77299</td>\n","      <td>213</td>\n","      <td>False</td>\n","      <td>video</td>\n","      <td>720</td>\n","      <td>720</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>24015</th>\n","      <td>Other</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>Other</td>\n","      <td>en</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>Buffer</td>\n","      <td>535</td>\n","      <td>553</td>\n","      <td>98</td>\n","      <td>7</td>\n","      <td>False</td>\n","      <td>photo</td>\n","      <td>768</td>\n","      <td>1200</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>18300</th>\n","      <td>Other</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>Other</td>\n","      <td>en</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>Twitter for Android</td>\n","      <td>907</td>\n","      <td>526</td>\n","      <td>99</td>\n","      <td>3</td>\n","      <td>False</td>\n","      <td>photo</td>\n","      <td>1057</td>\n","      <td>702</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>15563</th>\n","      <td>Other</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>Other</td>\n","      <td>en</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>Other</td>\n","      <td>2120</td>\n","      <td>4437</td>\n","      <td>98</td>\n","      <td>30</td>\n","      <td>False</td>\n","      <td>photo</td>\n","      <td>379</td>\n","      <td>720</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>22371</th>\n","      <td>Other</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>Other</td>\n","      <td>en</td>\n","      <td>3</td>\n","      <td>...</td>\n","      <td>Twitter for iPhone</td>\n","      <td>1920</td>\n","      <td>4285</td>\n","      <td>94</td>\n","      <td>67</td>\n","      <td>False</td>\n","      <td>photo</td>\n","      <td>2001</td>\n","      <td>1125</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>84</th>\n","      <td>Other</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>Other</td>\n","      <td>en</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>Other</td>\n","      <td>85885</td>\n","      <td>70720</td>\n","      <td>54</td>\n","      <td>955</td>\n","      <td>False</td>\n","      <td>photo</td>\n","      <td>606</td>\n","      <td>1007</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>5220</th>\n","      <td>Other</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>Other</td>\n","      <td>en</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>Other</td>\n","      <td>241</td>\n","      <td>563</td>\n","      <td>98</td>\n","      <td>7</td>\n","      <td>False</td>\n","      <td>photo</td>\n","      <td>1080</td>\n","      <td>1080</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>22827</th>\n","      <td>Other</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>Other</td>\n","      <td>en</td>\n","      <td>3</td>\n","      <td>...</td>\n","      <td>Twitter Web App</td>\n","      <td>1937</td>\n","      <td>4985</td>\n","      <td>95</td>\n","      <td>34</td>\n","      <td>False</td>\n","      <td>photo</td>\n","      <td>2304</td>\n","      <td>4096</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>5794</th>\n","      <td>Other</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>Other</td>\n","      <td>en</td>\n","      <td>50</td>\n","      <td>...</td>\n","      <td>Twitter for iPhone</td>\n","      <td>2519</td>\n","      <td>2564</td>\n","      <td>95</td>\n","      <td>38</td>\n","      <td>False</td>\n","      <td>photo</td>\n","      <td>2048</td>\n","      <td>1538</td>\n","      <td>Other</td>\n","    </tr>\n","    <tr>\n","      <th>4179</th>\n","      <td>Other</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>Other</td>\n","      <td>en</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>Hootsuite Inc.</td>\n","      <td>568</td>\n","      <td>552</td>\n","      <td>100</td>\n","      <td>15</td>\n","      <td>False</td>\n","      <td>photo</td>\n","      <td>630</td>\n","      <td>1200</td>\n","      <td>Other</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>500 rows √ó 25 columns</p>\n","</div>"],"text/plain":["        text  context_annotations_count  count_annotations  count_cashtags  \\\n","4427   Other                          3                  0               0   \n","24015  Other                          1                  0               0   \n","18300  Other                          1                  0               0   \n","15563  Other                          1                  2               0   \n","22371  Other                          1                  0               0   \n","...      ...                        ...                ...             ...   \n","84     Other                          6                  0               0   \n","5220   Other                          1                  0               0   \n","22827  Other                          1                  1               0   \n","5794   Other                          3                  0               0   \n","4179   Other                          1                  0               0   \n","\n","       count_hashtags  count_mentions  count_urls created_at_tweet lang  \\\n","4427                0               0           1            Other   en   \n","24015               3               0           2            Other   en   \n","18300               4               0           1            Other   en   \n","15563               0               0           2            Other   en   \n","22371               0               0           1            Other   en   \n","...               ...             ...         ...              ...  ...   \n","84                  0               0           2            Other   en   \n","5220                5               0           1            Other   en   \n","22827               1               0           1            Other   en   \n","5794                1               0           1            Other   en   \n","4179                2               0           2            Other   en   \n","\n","       likes  ...               source  author_followers_count  \\\n","4427       1  ...   Twitter for iPhone                    1945   \n","24015      1  ...               Buffer                     535   \n","18300      0  ...  Twitter for Android                     907   \n","15563      0  ...                Other                    2120   \n","22371      3  ...   Twitter for iPhone                    1920   \n","...      ...  ...                  ...                     ...   \n","84         0  ...                Other                   85885   \n","5220       1  ...                Other                     241   \n","22827      3  ...      Twitter Web App                    1937   \n","5794      50  ...   Twitter for iPhone                    2519   \n","4179       1  ...       Hootsuite Inc.                     568   \n","\n","       author_following_count author_tweet_count  author_listed_count  \\\n","4427                     1839              77299                  213   \n","24015                     553                 98                    7   \n","18300                     526                 99                    3   \n","15563                    4437                 98                   30   \n","22371                    4285                 94                   67   \n","...                       ...                ...                  ...   \n","84                      70720                 54                  955   \n","5220                      563                 98                    7   \n","22827                    4985                 95                   34   \n","5794                     2564                 95                   38   \n","4179                      552                100                   15   \n","\n","      author_verified  media_type  height  width  preview_image_url  \n","4427            False       video     720    720              Other  \n","24015           False       photo     768   1200              Other  \n","18300           False       photo    1057    702              Other  \n","15563           False       photo     379    720              Other  \n","22371           False       photo    2001   1125              Other  \n","...               ...         ...     ...    ...                ...  \n","84              False       photo     606   1007              Other  \n","5220            False       photo    1080   1080              Other  \n","22827           False       photo    2304   4096              Other  \n","5794            False       photo    2048   1538              Other  \n","4179            False       photo     630   1200              Other  \n","\n","[500 rows x 25 columns]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# Question 6: Next, create a function that will bin the groups within categorical features that don't make up at least n percent of the\n","# rows in the dataset. There is an example in the chapter of this type of function.\n","def bin_groups(df, features=[], cutoff=0.05, replace_with='Other', messages=True):\n","    import pandas as pd\n","\n","    if len(features) == 0: features = df.columns\n","\n","    for feat in features:\n","      if feat in df.columns:\n","        if not pd.api.types.is_numeric_dtype(df[feat]):\n","          other_list = df[feat].value_counts()[df[feat].value_counts() / df.shape[0] < cutoff].index\n","          df.loc[df[feat].isin(other_list), feat] = replace_with\n","          if messages and len(other_list) > 0: print(f'{feat} has been binned by setting {other_list} to {replace_with}')\n","      else:\n","        if messages: print(f'{feat} not found in the DataFrame provided. No binning performed')\n","\n","    return df\n","\n","bin_groups(df)\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":459,"status":"ok","timestamp":1709764190002,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":420},"id":"kqvuQgBxdgte","outputId":"9e402bed-d623-456d-bc68-1b4bcf641327"},"outputs":[{"name":"stdout","output_type":"stream","text":["The dataset has 24037 rows and 25 columns\n","created_at_tweet has been binned by setting Index(['2020-04-17T19:12:34.000Z', '2020-04-24T08:15:06.000Z',\n","       '2021-04-02T07:35:00.000Z', '2020-04-15T14:00:00.000Z',\n","       '2020-08-14T08:54:17.000Z', '2021-06-04T09:57:07.000Z',\n","       '2020-11-30T00:01:03.000Z', '2020-05-20T20:35:04.000Z',\n","       '2020-05-21T09:14:31.000Z', '2020-05-04T09:05:23.000Z',\n","       ...\n","       '2020-08-11T21:45:14.000Z', '2020-07-26T17:12:21.000Z',\n","       '2020-04-02T10:59:05.000Z', '2020-05-23T06:06:47.000Z',\n","       '2021-05-24T15:58:58.000Z', '2020-11-29T09:00:12.000Z',\n","       '2020-03-07T23:59:03.000Z', '2020-08-02T16:50:05.000Z',\n","       '2020-04-30T23:18:08.000Z', '2020-04-15T16:15:13.000Z'],\n","      dtype='object', name='created_at_tweet', length=500) to Other\n","lang has been binned by setting Index(['tl', 'in', 'es', 'cy', 'fr', 'ro'], dtype='object', name='lang') to Other\n","source has been binned by setting Index(['Zoho Social', 'Crowdfire App', 'CoSchedule', 'Sendible',\n","       'EveryoneSocial', 'SocialBee.io v2', 'Fanbooster by Traject', 'Grabyo',\n","       'GWP Digital', 'LaterMedia', 'Twitter for Advertisers.', 'Orlo',\n","       'True Anthem', 'Autism_twitter', 'Constant Contact - Social Posts',\n","       'SocialFlow', 'Echobox', 'ContentCal Studio', 'Publer.io',\n","       'iweller.com', 'Tweetbot for iŒüS', 'eClincher', 'Missinglettr',\n","       'SocialNewsDesk'],\n","      dtype='object', name='source') to Other\n","created_at_tweet has been binned by setting Index(['2020-04-17T19:12:34.000Z', '2020-04-24T08:15:06.000Z',\n","       '2021-04-02T07:35:00.000Z', '2020-04-15T14:00:00.000Z',\n","       '2020-08-14T08:54:17.000Z', '2021-06-04T09:57:07.000Z',\n","       '2020-11-30T00:01:03.000Z', '2020-05-20T20:35:04.000Z',\n","       '2020-05-21T09:14:31.000Z', '2020-05-04T09:05:23.000Z',\n","       ...\n","       '2020-08-11T21:45:14.000Z', '2020-07-26T17:12:21.000Z',\n","       '2020-04-02T10:59:05.000Z', '2020-05-23T06:06:47.000Z',\n","       '2021-05-24T15:58:58.000Z', '2020-11-29T09:00:12.000Z',\n","       '2020-03-07T23:59:03.000Z', '2020-08-02T16:50:05.000Z',\n","       '2020-04-30T23:18:08.000Z', '2020-04-15T16:15:13.000Z'],\n","      dtype='object', name='created_at_tweet', length=500) to Other\n","lang has been binned by setting Index(['tl', 'in', 'es', 'cy', 'fr', 'ro'], dtype='object', name='lang') to Other\n","source has been binned by setting Index(['dlvr.it', 'Twitter for iPad', 'IFTTT', 'The Social Jukebox',\n","       'Twitter Media Studio', 'Zoho Social', 'Crowdfire App', 'CoSchedule',\n","       'Sendible', 'EveryoneSocial', 'SocialBee.io v2',\n","       'Fanbooster by Traject', 'Grabyo', 'GWP Digital', 'LaterMedia',\n","       'Twitter for Advertisers.', 'Orlo', 'True Anthem', 'Autism_twitter',\n","       'Constant Contact - Social Posts', 'SocialFlow', 'Echobox',\n","       'ContentCal Studio', 'Publer.io', 'iweller.com', 'Tweetbot for iŒüS',\n","       'eClincher', 'Missinglettr', 'SocialNewsDesk'],\n","      dtype='object', name='source') to Other\n","created_at_tweet has been binned by setting Index(['2020-04-17T19:12:34.000Z', '2020-04-24T08:15:06.000Z',\n","       '2021-04-02T07:35:00.000Z', '2020-04-15T14:00:00.000Z',\n","       '2020-08-14T08:54:17.000Z', '2021-06-04T09:57:07.000Z',\n","       '2020-11-30T00:01:03.000Z', '2020-05-20T20:35:04.000Z',\n","       '2020-05-21T09:14:31.000Z', '2020-05-04T09:05:23.000Z',\n","       ...\n","       '2020-08-11T21:45:14.000Z', '2020-07-26T17:12:21.000Z',\n","       '2020-04-02T10:59:05.000Z', '2020-05-23T06:06:47.000Z',\n","       '2021-05-24T15:58:58.000Z', '2020-11-29T09:00:12.000Z',\n","       '2020-03-07T23:59:03.000Z', '2020-08-02T16:50:05.000Z',\n","       '2020-04-30T23:18:08.000Z', '2020-04-15T16:15:13.000Z'],\n","      dtype='object', name='created_at_tweet', length=500) to Other\n","lang has been binned by setting Index(['und', 'tl', 'in', 'es', 'cy', 'fr', 'ro'], dtype='object', name='lang') to Other\n","source has been binned by setting Index(['TweetDeck', 'dlvr.it', 'Twitter for iPad', 'IFTTT',\n","       'The Social Jukebox', 'Twitter Media Studio', 'Zoho Social',\n","       'Crowdfire App', 'CoSchedule', 'Sendible', 'EveryoneSocial',\n","       'SocialBee.io v2', 'Fanbooster by Traject', 'Grabyo', 'GWP Digital',\n","       'LaterMedia', 'Twitter for Advertisers.', 'Orlo', 'True Anthem',\n","       'Autism_twitter', 'Constant Contact - Social Posts', 'SocialFlow',\n","       'Echobox', 'ContentCal Studio', 'Publer.io', 'iweller.com',\n","       'Tweetbot for iŒüS', 'eClincher', 'Missinglettr', 'SocialNewsDesk'],\n","      dtype='object', name='source') to Other\n","1% Binning - Source Value Counts:\n"," source\n","Twitter Web App         131\n","Twitter for iPhone       99\n","Twitter for Android      59\n","Hootsuite Inc.           50\n","Other                    36\n","Sprout Social            35\n","Buffer                   33\n","TweetDeck                21\n","dlvr.it                   9\n","IFTTT                     7\n","Twitter for iPad          7\n","The Social Jukebox        7\n","Twitter Media Studio      6\n","Name: count, dtype: int64\n","\n","2% Binning - Source Value Counts:\n"," source\n","Twitter Web App        131\n","Twitter for iPhone      99\n","Other                   72\n","Twitter for Android     59\n","Hootsuite Inc.          50\n","Sprout Social           35\n","Buffer                  33\n","TweetDeck               21\n","Name: count, dtype: int64\n","\n","5% Binning - Source Value Counts:\n"," source\n","Twitter Web App        131\n","Twitter for iPhone      99\n","Other                   93\n","Twitter for Android     59\n","Hootsuite Inc.          50\n","Sprout Social           35\n","Buffer                  33\n","Name: count, dtype: int64\n"]}],"source":["# Question 7: Now, let's execute the function you created in the prior step within the entire pipeline using 5 percent as the cutoff value\n","# for the minimum group percent of rows that are required to keep the value without binning it to \"Other\". Save that binned\n","# version of the entire dataset as a new DataFrame. Then, call the function again with the entire dataset using a cutoff\n","# parameter of 2 percent and save that as a new DataFrame separate from the 5 percent version. Do this again for a\n","# version where anything under 1 percent is binned. We are going to compare these three datasets later just like we\n","# did in the chapter. Please note that you will probably need to use a .copy() of your DataFrame when you create the\n","# three different versions of the dataset to avoid memory issues. For example:\n","\n","# new_2_percent_df = function_name(df.copy(), cutoff=0.02)\n","\n","# The pipeline order should look like this pseudocode:\n","\n","# import entire dataset\n","# generate reach and drop the features specified in that function\n","# save a new df based on 1 percent binning\n","# save a new df based on 2 percent binning\n","# save a new df based on 5 percent binning\n","\n","# Once you've run the functions in the pipeline and saved the three new versions of the dataset based on the 1, 2, and 5\n","# percent binning, print out the value counts of the \"source\" feature from each of those three DataFrames\n","df = importData('tw_tweets_users_media.csv')\n","\n","processed_data = process_dataset(df)\n","\n","df_1 = bin_groups(processed_data.copy(), cutoff=0.01)\n","df_2 = bin_groups(processed_data.copy(), cutoff=0.02)\n","df_5 = bin_groups(processed_data.copy(), cutoff=0.05)\n","\n","print(\"1% Binning - Source Value Counts:\\n\", df_1['source'].value_counts())\n","print(\"\\n2% Binning - Source Value Counts:\\n\", df_2['source'].value_counts())\n","print(\"\\n5% Binning - Source Value Counts:\\n\", df_5['source'].value_counts())"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":122,"status":"ok","timestamp":1709764192745,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":420},"id":"jQrKwVnDiAV0"},"outputs":[],"source":["# Question 8: The final cleaning step we need to perform is to convert the date field 'created_at_tweet' into usable features. You\n","# may remember doing something similar in a prior assignment. This time, we are going to create a function to accomplish\n","# this task. Make a function that will convert a date column into five new columns for: hour of the day, day of the week,\n","# day of the month, month of the year, and year. You should delete the original date so that it does not get included in\n","# in the modeling phase. You should also convert the weekday into a categorical data type. If you return the weekday in text\n","# form (e.g. Monday, Tuesday) then this will happen automatically. If you create it as a 0-6 or 1-7 feature, then you will\n","# have to manually cast it to an 'object'.\n","\n","def convert_date_column(df, date_column):\n","    import pandas as pd\n","\n","    # Convert the date column to datetime\n","    df[date_column] = pd.to_datetime(df[date_column])\n","\n","    # Extract features from the date column\n","    df['hour_of_day'] = df[date_column].dt.hour\n","    df['day_of_month'] = df[date_column].dt.day\n","    df['month_of_year'] = df[date_column].dt.month\n","    df['year'] = df[date_column].dt.year\n","    df['day_of_week'] = df[date_column].dt.day_name()\n","\n","    # Drop the original date column\n","    df = df.drop(date_column, axis=1)\n","\n","    return df\n"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":304,"status":"ok","timestamp":1709764202939,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":420},"id":"xifikOFWmLi7","outputId":"af87867e-e19b-47bb-c2ad-e5f3a72be586"},"outputs":[{"name":"stdout","output_type":"stream","text":["The dataset has 24037 rows and 25 columns\n","lang has been binned by setting Index(['tl', 'in', 'es', 'cy', 'fr', 'ro'], dtype='object', name='lang') to Other\n","source has been binned by setting Index(['Zoho Social', 'Crowdfire App', 'CoSchedule', 'Sendible',\n","       'EveryoneSocial', 'SocialBee.io v2', 'Fanbooster by Traject', 'Grabyo',\n","       'GWP Digital', 'LaterMedia', 'Twitter for Advertisers.', 'Orlo',\n","       'True Anthem', 'Autism_twitter', 'Constant Contact - Social Posts',\n","       'SocialFlow', 'Echobox', 'ContentCal Studio', 'Publer.io',\n","       'iweller.com', 'Tweetbot for iŒüS', 'eClincher', 'Missinglettr',\n","       'SocialNewsDesk'],\n","      dtype='object', name='source') to Other\n","(500, 23)\n","lang has been binned by setting Index(['tl', 'in', 'es', 'cy', 'fr', 'ro'], dtype='object', name='lang') to Other\n","source has been binned by setting Index(['dlvr.it', 'Twitter for iPad', 'IFTTT', 'The Social Jukebox',\n","       'Twitter Media Studio', 'Zoho Social', 'Crowdfire App', 'CoSchedule',\n","       'Sendible', 'EveryoneSocial', 'SocialBee.io v2',\n","       'Fanbooster by Traject', 'Grabyo', 'GWP Digital', 'LaterMedia',\n","       'Twitter for Advertisers.', 'Orlo', 'True Anthem', 'Autism_twitter',\n","       'Constant Contact - Social Posts', 'SocialFlow', 'Echobox',\n","       'ContentCal Studio', 'Publer.io', 'iweller.com', 'Tweetbot for iŒüS',\n","       'eClincher', 'Missinglettr', 'SocialNewsDesk'],\n","      dtype='object', name='source') to Other\n","(500, 23)\n","lang has been binned by setting Index(['und', 'tl', 'in', 'es', 'cy', 'fr', 'ro'], dtype='object', name='lang') to Other\n","source has been binned by setting Index(['TweetDeck', 'dlvr.it', 'Twitter for iPad', 'IFTTT',\n","       'The Social Jukebox', 'Twitter Media Studio', 'Zoho Social',\n","       'Crowdfire App', 'CoSchedule', 'Sendible', 'EveryoneSocial',\n","       'SocialBee.io v2', 'Fanbooster by Traject', 'Grabyo', 'GWP Digital',\n","       'LaterMedia', 'Twitter for Advertisers.', 'Orlo', 'True Anthem',\n","       'Autism_twitter', 'Constant Contact - Social Posts', 'SocialFlow',\n","       'Echobox', 'ContentCal Studio', 'Publer.io', 'iweller.com',\n","       'Tweetbot for iŒüS', 'eClincher', 'Missinglettr', 'SocialNewsDesk'],\n","      dtype='object', name='source') to Other\n","(500, 23)\n"]}],"source":["# Question 9: Call the date function you created in the prior step to convert 'created_at_tweet' into three new columns. However,\n","# to make our pipeline more efficient we should add this function BEFORE we create the three different binned version\n","# based on the 1, 2, and 5 percent cutoffs. So, execute the entire pipeline here with the date function in the correct\n","# place:\n","\n","# import entire dataset\n","# generate reach and drop the features specified in that function\n","# call the date function to convert 'created_at_tweet' into three new columns and drop the original column\n","# save a new df based on 1 percent binning\n","# save a new df based on 2 percent binning\n","# save a new df based on 5 percent binning\n","\n","df = importData('tw_tweets_users_media.csv')\n","\n","processed_data = process_dataset(df)\n","\n","processed_data = convert_date_column(processed_data, 'created_at_tweet')\n","\n","df_1 = bin_groups(processed_data.copy(), cutoff=0.01)\n","print(df_1.shape)\n","\n","df_2 = bin_groups(processed_data.copy(), cutoff=0.02)\n","print(df_2.shape)\n","\n","df_5 = bin_groups(processed_data.copy(), cutoff=0.05)\n","print(df_5.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"wSEONFKNx9T7"},"source":["# Modeling and Evaluation"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":99,"status":"ok","timestamp":1709766864225,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":420},"id":"H-F7TrUkwtVf"},"outputs":[],"source":["# Question 10: Now it is time for modeling. Create a function that works just like the fit_cv_regression() example\n","# in the book. Note that this means you will also need to either a) also copy in the functions required to generate the\n","# X and y datasets and perform dummy coding from the book, or b) write the code to perform those steps within the revised\n","# fit_cv_regression() function you create here. You will not need to address missing data or a train/test split since\n","# we are just going to use the cross-validation technique in this function.\n","\n","# Modify this function so that rather than specifying a family of algorithms to try (e.g. 'linear', 'ensemble', or\n","# 'other'), it simply tries all of the following algorithms: Ridge, LassoLars, RandomForestRegressor, and\n","# GradientBoostingRegressor. It should print out the R2 scores if the user who calls the function indicates through\n","# a parameter (e.g. messages=True). It should also return a trained model based on the highest R squared score.\n","\n","from sklearn.model_selection import cross_val_score\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","from sklearn.linear_model import Ridge, LassoLars\n","import pandas as pd\n","import numpy as np\n","\n","def Xandy(df, label):\n","  import pandas as pd\n","  y = df[label]\n","  X = df.drop(columns=[label])\n","  return X, y\n","\n","def dummy_code(X):\n","  import pandas as pd\n","  X = pd.get_dummies(X, drop_first=True)\n","  return X\n","\n","def minmax(X):\n","  import pandas as pd\n","  from sklearn.preprocessing import MinMaxScaler\n","  X = pd.DataFrame(MinMaxScaler().fit_transform(X.copy()), columns=X.columns, index=X.index)\n","  return X\n","\n","def impute_KNN(df, label, neighbors=5):\n","  from sklearn.impute import KNNImputer\n","  import pandas as pd\n","  X, y = Xandy(df, label)\n","  X = dummy_code(X.copy())\n","  X = minmax(X.copy())\n","  imp = KNNImputer(n_neighbors=neighbors, weights=\"uniform\")\n","  X = pd.DataFrame(imp.fit_transform(X), columns=X.columns, index=X.index)\n","  return X.merge(y, left_index=True, right_index=True)\n","\n","def fit_cv_regression(df, label, k=5, repeat=True, random_state=1, messages=True):\n","  from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n","  import pandas as pd\n","  from numpy import mean\n","\n","  X, y = Xandy(df, label)\n","  X = dummy_code(X.copy())\n","\n","  if repeat:\n","    cv = RepeatedKFold(n_splits=k, n_repeats=5, random_state=1)\n","  else:\n","    cv = KFold(n_splits=k, random_state=1)\n","\n","  from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","  from sklearn.linear_model import Ridge, LassoLars\n","\n","  model_rfc = RandomForestRegressor(random_state=random_state)\n","  model_ridge = Ridge(random_state=random_state)\n","  model_gbc = GradientBoostingRegressor(random_state=random_state)\n","  model_log = LassoLars(random_state=random_state)\n","\n","  scores_rfc = cross_val_score(model_rfc, X, y, scoring='r2', cv=cv, n_jobs=-1)\n","  scores_ridge = cross_val_score(model_ridge, X, y, scoring='r2', cv=cv, n_jobs=-1)\n","  scores_gbc = cross_val_score(model_gbc, X, y, scoring='r2', cv=cv, n_jobs=-1)\n","  scores_log = cross_val_score(model_log, X, y, scoring='r2', cv=cv, n_jobs=-1)\n","\n","  scores = {mean(scores_rfc):model_rfc,\n","            mean(scores_ridge):model_ridge,\n","            mean(scores_gbc):model_gbc,\n","            mean(scores_log):model_log}\n","\n","  if messages:\n","    for score in scores:\n","        print(score)\n","\n","  return scores[max(scores.keys())].fit(X, y)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"executionInfo":{"elapsed":43490,"status":"ok","timestamp":1709766014425,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":420},"id":"ewun_FS22IIi","outputId":"544da2a7-28e0-4587-fa82-9575ae66aaa2"},"outputs":[{"name":"stdout","output_type":"stream","text":["lang has been binned by setting Index(['tl', 'es', 'in', 'ro', 'cy', 'fr', 'sv', 'de', 'lt', 'tr', 'ta'], dtype='object', name='lang') to Other\n","source has been binned by setting Index(['Twitter for iPad', 'Crowdfire App', 'Zoho Social', 'CoSchedule',\n","       'SocialBee.io v2', 'Fanbooster by Traject', 'Autism_twitter',\n","       'LaterMedia', 'Twitter for Advertisers.', 'WordPress.com', 'Sendible',\n","       'EveryoneSocial', 'Missinglettr', 'iweller.com', 'Lightful',\n","       'ContentCal Studio', 'MeetEdgar', 'Kyle2U Mental Health', 'Apphi',\n","       'HeyOrca', 'PNS_FL', 'Triberr', 'Gain Platform', 'Meltwater Social',\n","       'Agorapulse app', 'Mirage News Posts', 'Lara CNA', 'ENCtweets',\n","       'Semrush Social Media Tool', 'SmarterQueue', 'Sprinklr',\n","       'Twitter Web Client', 'Woofy Social Media Scheduler', 'True Anthem',\n","       'MavSocial Ads', 'Jenkers Eng Posting', 'Publer.io', 'Grabyo',\n","       'SocialNewsDesk', 'Echobox', 'eClincher', 'Tweetbot for iŒüS',\n","       'SocialFlow', 'GWP Digital', 'Constant Contact - Social Posts', 'Orlo',\n","       'Falcon Social Media Management ', 'mmautism', 'World Nws',\n","       'Blog2Social APP', 'of today', 'Constant Contact'],\n","      dtype='object', name='source') to Other\n","(1000, 23)\n","lang has been binned by setting Index(['und', 'tl', 'es', 'in', 'ro', 'cy', 'fr', 'sv', 'de', 'lt', 'tr',\n","       'ta'],\n","      dtype='object', name='lang') to Other\n","source has been binned by setting Index(['IFTTT', 'The Social Jukebox', 'Twitter Media Studio',\n","       'Twitter for iPad', 'Crowdfire App', 'Zoho Social', 'CoSchedule',\n","       'SocialBee.io v2', 'Fanbooster by Traject', 'Autism_twitter',\n","       'LaterMedia', 'Twitter for Advertisers.', 'WordPress.com', 'Sendible',\n","       'EveryoneSocial', 'Missinglettr', 'iweller.com', 'Lightful',\n","       'ContentCal Studio', 'MeetEdgar', 'Kyle2U Mental Health', 'Apphi',\n","       'HeyOrca', 'PNS_FL', 'Triberr', 'Gain Platform', 'Meltwater Social',\n","       'Agorapulse app', 'Mirage News Posts', 'Lara CNA', 'ENCtweets',\n","       'Semrush Social Media Tool', 'SmarterQueue', 'Sprinklr',\n","       'Twitter Web Client', 'Woofy Social Media Scheduler', 'True Anthem',\n","       'MavSocial Ads', 'Jenkers Eng Posting', 'Publer.io', 'Grabyo',\n","       'SocialNewsDesk', 'Echobox', 'eClincher', 'Tweetbot for iŒüS',\n","       'SocialFlow', 'GWP Digital', 'Constant Contact - Social Posts', 'Orlo',\n","       'Falcon Social Media Management ', 'mmautism', 'World Nws',\n","       'Blog2Social APP', 'of today', 'Constant Contact'],\n","      dtype='object', name='source') to Other\n","(1000, 23)\n","lang has been binned by setting Index(['und', 'tl', 'es', 'in', 'ro', 'cy', 'fr', 'sv', 'de', 'lt', 'tr',\n","       'ta'],\n","      dtype='object', name='lang') to Other\n","source has been binned by setting Index(['TweetDeck', 'dlvr.it', 'IFTTT', 'The Social Jukebox',\n","       'Twitter Media Studio', 'Twitter for iPad', 'Crowdfire App',\n","       'Zoho Social', 'CoSchedule', 'SocialBee.io v2', 'Fanbooster by Traject',\n","       'Autism_twitter', 'LaterMedia', 'Twitter for Advertisers.',\n","       'WordPress.com', 'Sendible', 'EveryoneSocial', 'Missinglettr',\n","       'iweller.com', 'Lightful', 'ContentCal Studio', 'MeetEdgar',\n","       'Kyle2U Mental Health', 'Apphi', 'HeyOrca', 'PNS_FL', 'Triberr',\n","       'Gain Platform', 'Meltwater Social', 'Agorapulse app',\n","       'Mirage News Posts', 'Lara CNA', 'ENCtweets',\n","       'Semrush Social Media Tool', 'SmarterQueue', 'Sprinklr',\n","       'Twitter Web Client', 'Woofy Social Media Scheduler', 'True Anthem',\n","       'MavSocial Ads', 'Jenkers Eng Posting', 'Publer.io', 'Grabyo',\n","       'SocialNewsDesk', 'Echobox', 'eClincher', 'Tweetbot for iŒüS',\n","       'SocialFlow', 'GWP Digital', 'Constant Contact - Social Posts', 'Orlo',\n","       'Falcon Social Media Management ', 'mmautism', 'World Nws',\n","       'Blog2Social APP', 'of today', 'Constant Contact'],\n","      dtype='object', name='source') to Other\n","(1000, 23)\n","5 Percent Binning\n","0.9440521032284055\n","0.5033751994645784\n","0.9488381311831443\n","0.4985189406579493\n","2 Percent Binning\n","0.9448197351267789\n","0.49832664514546493\n","0.94813370644692\n","0.4964017777974882\n","1 Percent Binning\n","0.9457177403050782\n","0.493379105856278\n","0.9482407985349478\n","0.4849359827644682\n"]}],"source":["# Question 11: Let's call your revised version of fit_cv_regression() (or whatever you decided to name it) once for each of our three\n","# binned versions of the dataset based on the 1, 2, and 5 percent cutoffs that also includes the functions that drop\n","# the features we can't use and generates the 'reach' label and the funtion to convert dates. However, because we have\n","# over 24k records, these pipelines could take a while. Therefore, let's sub-sample the dataset down to 1000 records.\n","# Use a random seed of 1 everywhere it is needed in the fit_cv_regression function including the cross-validator\n","# object and each algorithm. Specify a repeated k fold cross-validation with 5 folds and 5 repeats.\n","\n","# The pipeline should look something like this:\n","\n","# import a sub-sample of dataset including only 1000 records (remember that random seed = 1)\n","# generate reach and drop the features specified in that function\n","# call the date function to convert 'created_at_tweet' into three new columns and drop the original column\n","# save a new df based on 1 percent binning\n","# save a new df based on 2 percent binning\n","# save a new df based on 5 percent binning\n","# fit_cv_regression based on 5 folds with 5 repeats using the 5 percent binned DataFrame and print out all R2 values\n","# fit_cv_regression based on 5 folds with 5 repeats using the 2 percent binned DataFrame and print out all R2 values\n","# fit_cv_regression based on 5 folds with 5 repeats using the 1 percent binned DataFrame and print out all R2 values\n","\n","df = importData('tw_tweets_users_media.csv', sample_size=1000, show_shape=False)\n","\n","processed_data = process_dataset(df)    \n","\n","processed_data = convert_date_column(processed_data, 'created_at_tweet')\n","\n","df_1 = bin_groups(processed_data.copy(), cutoff=0.01)\n","print(df_1.shape)\n","df_2 = bin_groups(processed_data.copy(), cutoff=0.02)\n","print(df_2.shape)\n","df_5 = bin_groups(processed_data.copy(), cutoff=0.05)\n","print(df_5.shape)\n","\n","print(\"5 Percent Binning\")\n","best_model_5 = fit_cv_regression(df_5, k=5, label = 'reach', repeat=True, random_state=1, messages=True)\n","\n","print(\"2 Percent Binning\")\n","best_model_2 = fit_cv_regression(df_2, k=5, label = 'reach', repeat=True, random_state=1, messages=True)\n","\n","print(\"1 Percent Binning\")\n","best_model_1 = fit_cv_regression(df_1, k=5, label = 'reach', repeat=True, random_state=1, messages=True)\n"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":359417,"status":"ok","timestamp":1709767229418,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":420},"id":"bM1XxnHH47Ri","outputId":"adcfbbc0-1c7a-41a6-b3ae-db61955e757f"},"outputs":[{"name":"stdout","output_type":"stream","text":["lang has been binned by setting Index(['und', 'tl', 'es', 'in', 'ro', 'cy', 'fr', 'sv', 'de', 'lt', 'tr',\n","       'ta'],\n","      dtype='object', name='lang') to Other\n","source has been binned by setting Index(['TweetDeck', 'dlvr.it', 'IFTTT', 'The Social Jukebox',\n","       'Twitter Media Studio', 'Twitter for iPad', 'Crowdfire App',\n","       'Zoho Social', 'CoSchedule', 'SocialBee.io v2', 'Fanbooster by Traject',\n","       'Autism_twitter', 'LaterMedia', 'Twitter for Advertisers.',\n","       'WordPress.com', 'Sendible', 'EveryoneSocial', 'Missinglettr',\n","       'iweller.com', 'Lightful', 'ContentCal Studio', 'MeetEdgar',\n","       'Kyle2U Mental Health', 'Apphi', 'HeyOrca', 'PNS_FL', 'Triberr',\n","       'Gain Platform', 'Meltwater Social', 'Agorapulse app',\n","       'Mirage News Posts', 'Lara CNA', 'ENCtweets',\n","       'Semrush Social Media Tool', 'SmarterQueue', 'Sprinklr',\n","       'Twitter Web Client', 'Woofy Social Media Scheduler', 'True Anthem',\n","       'MavSocial Ads', 'Jenkers Eng Posting', 'Publer.io', 'Grabyo',\n","       'SocialNewsDesk', 'Echobox', 'eClincher', 'Tweetbot for iŒüS',\n","       'SocialFlow', 'GWP Digital', 'Constant Contact - Social Posts', 'Orlo',\n","       'Falcon Social Media Management ', 'mmautism', 'World Nws',\n","       'Blog2Social APP', 'of today', 'Constant Contact'],\n","      dtype='object', name='source') to Other\n","0.9440521032284055\n","0.5033751994645784\n","0.9488381311831443\n","0.4985189406579493\n","Model saved as saved_model_5_percent_gradientboosting.sav\n"]}],"source":["# Question 12: Examine the 12 model fit scores from those three pipeline runs you performed in the last step. Identify the best model\n","# fit. Write a function (like the one in the book) to save a trained model based on the binned dataset (1, 2, or 5\n","# percent) and algorithm (Ridge, LassoLars, RandomForest, or GradientBoosting) that gives the highest R squared score.\n","# This function should use the pickle package to save the trained model.\n","\n","# Rerun the entire pipeline using ONLY the binned version of the dataset AND algorithm that gives the highest model fit\n","# score. For example, if a LassoLars regression using the 2 percent binned dataset gives the highest R2, then store that\n","# model and save it as an exported file using the function you created called, 'saved_model.sav'.\n","\n","# With this pipeline run, use the entire imported dataset with all 24k+ records. It will take a while, so you may want to test your\n","# pipeline first with a smaller dataset. Continue to use all of the other settings as specified in prior questions: random seed = 1\n","# everywhere, 5 folds, 5 repeats.\n","\n","\n","import pickle\n","\n","def save_model(model, filename='saved_model.sav'):\n","    with open(filename, 'wb') as file:\n","        pickle.dump(model, file)\n","    print(f\"Model saved as {filename}\")\n","\n","\n","df = importData('tw_tweets_users_media.csv', sample_size=1000, show_shape=False)  \n","\n","df = process_dataset(df)\n","df = convert_date_column(df, 'created_at_tweet')\n","df_5 = bin_groups(df.copy(), cutoff=0.05)\n","\n","# Fit the model again using Gradient Boosting\n","model = fit_cv_regression(df_5, label='reach', k=5, repeat=True, random_state=1, messages=True)\n","\n","# Save the model\n","save_model(model, filename='saved_model_5_percent_gradientboosting.sav')"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":120,"status":"ok","timestamp":1709767451341,"user":{"displayName":"Mark Keith","userId":"01471376858192886314"},"user_tz":420},"id":"mDSw-YC38IqV","outputId":"6757103d-1db9-4c72-9237-754d906943f3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicted reach: [22.49120964]\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names\n","  warnings.warn(\n"]}],"source":["# Question 13: Finally, generate a function that will read the file with a saved model based on the path you pass into the funtion. Call\n","# the function and store the file you read into an object that you use to make an out-of-sample prediction using the following\n","# data:\n","\n","# context_annotations_count                 6\n","# count_annotations                         2\n","# count_cashtags                            4\n","# count_hashtags                           25\n","# count_mentions                           12\n","# count_urls                                1\n","# referenced_tweet_count                    0\n","# author_followers_count                23367\n","# author_following_count                 5071\n","# author_tweet_count                       98\n","# author_listed_count                      50\n","# author_verified                       False\n","# height                                  550\n","# width                                   963\n","# created_at_tweet_hour                    16\n","# created_at_tweet_day                      1\n","# created_at_tweet_month                   11\n","# created_at_tweet_year                  2020\n","# created_at_tweet_weekday:            Sunday\n","# lang:                                    en\n","# source:                     Twitter Web App\n","# media_type:                           photo\n","\n","# HINT: After you read in the saved model file, access the .feature_names_in_ property to see what order the values go into the\n","# model so that you get the values above in the right order with dummy codes.\n","\n","import pickle\n","\n","def load_model_and_predict(input_data, model_path):\n","    # Load the saved model\n","    with open(model_path, 'rb') as file:\n","        saved_model = pickle.load(file)\n","\n","    # Extract feature names from the model\n","    feature_names = saved_model.feature_names_in_\n","\n","    # Arrange the input data in the correct order\n","    input_data_ordered = []\n","    for feature in feature_names:\n","        if feature in input_data:\n","            input_data_ordered.append(input_data[feature])\n","        else:\n","            input_data_ordered.append(0)  # If a feature is missing, fill it with 0\n","\n","    # Make prediction\n","    prediction = saved_model.predict([input_data_ordered])\n","\n","    return prediction\n","\n","# Input data for prediction\n","input_data = {\n","    'context_annotations_count': 6,\n","    'count_annotations': 2,\n","    'count_cashtags': 4,\n","    'count_hashtags': 25,\n","    'count_mentions': 12,\n","    'count_urls': 1,\n","    'referenced_tweet_count': 0,\n","    'author_followers_count': 23367,\n","    'author_following_count': 5071,\n","    'author_tweet_count': 98,\n","    'author_listed_count': 50,\n","    'author_verified': False,\n","    'height': 550,\n","    'width': 963,\n","    'created_at_tweet_hour': 16,\n","    'created_at_tweet_day': 1,\n","    'created_at_tweet_month': 11,\n","    'created_at_tweet_year': 2020,\n","    'created_at_tweet_weekday': 'Sunday',\n","    'lang': 'en',\n","    'source': 'Twitter Web App',\n","    'media_type': 'photo'\n","}\n","\n","# Path to the saved model file\n","model_path = 'saved_model_5_percent_gradientboosting.sav'\n","\n","# Make prediction using the loaded model\n","prediction = load_model_and_predict(input_data, model_path)\n","print(\"Predicted reach:\", prediction)\n","\n"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"JE2xUJpD9mvS"},"outputs":[],"source":["# Question 14: Now that you have a pipeline that can be used to make predictions on new data, let's create a function that will minimize overfitting by removing features that are not important. This function should take in a trained model, the data set, and the name of the label. It should return the entire dataset including the reduced set of the most important features and the y label. It should print out the number of features before and after the feature selection unless the caller sets messages=False as a function parameter.\n","def select_features(df, label, model, max='auto'):\n","  from sklearn.feature_selection import SelectFromModel\n","  import pandas as pd\n","\n","\n","  y = df[label]\n","  X = df.drop(columns=[label])\n","  X = dummy_code(X)\n","\n","  print(len(model.feature_names_in_))\n","  print(len(X.columns))\n","\n","  if max != 'auto':\n","    sel = SelectFromModel(model, prefit=True, max_features=round(max*df.drop(columns=[label]).shape[1]))\n","  else:\n","    sel = SelectFromModel(model, prefit=True)\n","\n","\n","  sel.transform(X)\n","  columns = list(X.columns[sel.get_support()])\n","\n","  df = X[columns]\n","  df[label] = y\n","\n","  return df\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The dataset has 24037 rows and 25 columns\n","33\n","33\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n","  warnings.warn(\n","C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3832\\2337897766.py:24: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df[label] = y\n"]},{"name":"stdout","output_type":"stream","text":["Selected Feature Names after Feature Selection:\n","Index(['referenced_tweet_count', 'height', 'day_of_month', 'media_type_video'], dtype='object')\n","R2 Score after Feature Selection: 0.9984870392388859\n"]}],"source":["# Question 15: Call the function you just created using the last version of the pipeline you created with this new function inserted AFTER you generate and store the model. Use this function in the full version of the pipeline which should look something like this pseudocode:\n","\n","# 1. import the entire dataset\n","# 2. generate reach and drop the features specified in that function\n","# 3. call the date function to convert 'created_at_tweet' into three new columns and drop the original column\n","# 4. call the binning function to bin all remaining categorical features based on the cutoff that gave you the best model fit\n","# 5. run fit_cv_regression based on 5 folds with 5 repeats using the last version of the dataset\n","# 6. call the feature selection function to reduce the number of features; allow the function to print out the before-and-after feature counts\n","# 7. run fit_cv_regression based on 5 folds with 5 repeats using the last version of the dataset after feature selection\n","# don't worry about saving/deploying the model for this question\n","\n","# Print out the feature names (excluding the label) that are left after the feature selection. Also print out the R2 score of the model after the feature selection.\n","\n","\n","# Step 1: Import the entire dataset\n","df = importData('tw_tweets_users_media.csv')\n","\n","# Step 2: Process the dataset to generate 'reach' and drop specified features\n","processed_df = process_dataset(df)\n","\n","# Step 3: Convert 'created_at_tweet' into new date-time related columns\n","processed_df = convert_date_column(processed_df, 'created_at_tweet')\n","\n","# Step 4: Create binned versions of the dataset based on the cutoff that gave the best model fit\n","df_best_binned = bin_groups(processed_df.copy(), cutoff=0.05, messages=False)\n","\n","# Step 5: Fit models using the revised fit_cv_regression function\n","best_model = fit_cv_regression(df_best_binned, label='reach', k=5, repeat=True, messages=False)\n","\n","# Step 6: Perform feature selection to reduce the number of features\n","df_selected = select_features(df_best_binned, label='reach', model=best_model)\n","\n","# Step 7: Fit models again after feature selection\n","best_model_after_selection = fit_cv_regression(df_selected, label='reach', k=5, repeat=True, messages=False)\n","\n","# Print out the feature names (excluding the label) left after feature selection\n","selected_feature_names = df_selected.columns[:-1]  # Exclude the label column\n","print(\"Selected Feature Names after Feature Selection:\")\n","print(selected_feature_names)\n","\n","# Print out the R2 score of the model after feature selection\n","X_selected, y = Xandy(df_selected, label='reach')\n","r2_score_after_selection = best_model_after_selection.score(X_selected, y)\n","print(\"R2 Score after Feature Selection:\", r2_score_after_selection)\n","\n"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["# Question 16: Create a function to test many different regression algorithms. In particular, it should test the following algorithms: Ridge, LassoLars, BayesianRidge, TweedieRegressor based on a Poisson model, SVR, KNeighborsRegressor based on a 'distance' weight and n_neighbors=10, RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, and HistGradientBoostingRegressor all from the sklearn package as well as XGBRegressor from the XGBoost package (for XGBRegressor, do not set any hyperparameters other than random_state). It should only try those algorithms and no others. \n","\n","# Make this function automatically train and return the most accurate model from those options. Allow this function to use either a KFold or RepeatedKFold cross-validator. Make sure to use a random seed of 1 everywhere it is needed. Also, make sure to use the R2 score as the metric to determine the best model. This function should also print out the R2 scores if the user who calls the function indicates through a parameter (e.g. messages=True).\n","\n","from sklearn.model_selection import cross_val_score, KFold, RepeatedKFold\n","from sklearn.linear_model import Ridge, LassoLars, BayesianRidge\n","from sklearn.linear_model import TweedieRegressor\n","from sklearn.svm import SVR\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\n","from xgboost import XGBRegressor\n","\n","def test_regression_algorithms(X, y, cv='kf', messages=False):\n","    models = {\n","        \"Ridge\": Ridge(random_state=1),\n","        \"LassoLars\": LassoLars(random_state=1),\n","        \"BayesianRidge\": BayesianRidge(),\n","        \"TweedieRegressor\": TweedieRegressor(power=1, link='log'),\n","        \"SVR\": SVR(),\n","        \"KNeighborsRegressor\": KNeighborsRegressor(weights='distance', n_neighbors=10),\n","        \"RandomForestRegressor\": RandomForestRegressor(random_state=1),\n","        \"AdaBoostRegressor\": AdaBoostRegressor(random_state=1),\n","        \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=1),\n","        \"HistGradientBoostingRegressor\": HistGradientBoostingRegressor(random_state=1),\n","        \"XGBRegressor\": XGBRegressor(random_state=1)\n","    }\n","\n","    if cv == 'kf':\n","        cv_strategy = KFold(n_splits=5, shuffle=True, random_state=1)\n","    elif cv == 'rkf':\n","        cv_strategy = RepeatedKFold(n_splits=5, n_repeats=5, random_state=1)\n","    else:\n","        raise ValueError(\"Invalid cross-validation strategy. Use 'kf' or 'rkf'.\")\n","\n","    best_model = None\n","    best_score = float('-inf')\n","\n","    for name, model in models.items():\n","        scores = cross_val_score(model, X, y, cv=cv_strategy, scoring='r2')\n","        mean_score = scores.mean()\n","        if mean_score > best_score:\n","            best_score = mean_score\n","            best_model = model\n","\n","        if messages:\n","            print(f\"R2 score for {name}: {mean_score}\")\n","\n","    if messages:\n","        print(f\"\\nBest model: {best_model.__class__.__name__} with R2 score: {best_score}\")\n","\n","    return best_model\n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The dataset has 24037 rows and 25 columns\n","33\n","33\n","R2 score for Ridge: 0.4375426129589755\n","R2 score for LassoLars: 0.43495847854591174\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n","  warnings.warn(\n","C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_3832\\2337897766.py:24: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df[label] = y\n"]},{"name":"stdout","output_type":"stream","text":["R2 score for BayesianRidge: 0.4353680370347094\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_linear_loss.py:294: RuntimeWarning: invalid value encountered in matmul\n","  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n","C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_linear_loss.py:294: RuntimeWarning: invalid value encountered in matmul\n","  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n","C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_linear_loss.py:294: RuntimeWarning: invalid value encountered in matmul\n","  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n","C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_linear_loss.py:294: RuntimeWarning: invalid value encountered in matmul\n","  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n","C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_linear_loss.py:294: RuntimeWarning: invalid value encountered in matmul\n","  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n","C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_linear_loss.py:294: RuntimeWarning: invalid value encountered in matmul\n","  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n","C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_linear_loss.py:294: RuntimeWarning: invalid value encountered in matmul\n","  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n","C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_linear_loss.py:294: RuntimeWarning: invalid value encountered in matmul\n","  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n","C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_linear_loss.py:294: RuntimeWarning: invalid value encountered in matmul\n","  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n","C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_linear_loss.py:294: RuntimeWarning: invalid value encountered in matmul\n","  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n","C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_linear_loss.py:294: RuntimeWarning: invalid value encountered in matmul\n","  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n","C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_linear_loss.py:294: RuntimeWarning: invalid value encountered in matmul\n","  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n","C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_linear_loss.py:294: RuntimeWarning: invalid value encountered in matmul\n","  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n","C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_linear_loss.py:294: RuntimeWarning: invalid value encountered in matmul\n","  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n","C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_linear_loss.py:294: RuntimeWarning: invalid value encountered in matmul\n","  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n","C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_linear_loss.py:294: RuntimeWarning: invalid value encountered in matmul\n","  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n","C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_linear_loss.py:294: RuntimeWarning: invalid value encountered in matmul\n","  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n","C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_linear_loss.py:294: RuntimeWarning: invalid value encountered in matmul\n","  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n","C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_linear_loss.py:294: RuntimeWarning: invalid value encountered in matmul\n","  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n","C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_linear_loss.py:294: RuntimeWarning: invalid value encountered in matmul\n","  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n","C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_linear_loss.py:294: RuntimeWarning: invalid value encountered in matmul\n","  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n","C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_linear_loss.py:294: RuntimeWarning: invalid value encountered in matmul\n","  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n","C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_linear_loss.py:294: RuntimeWarning: invalid value encountered in matmul\n","  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n","C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_linear_loss.py:294: RuntimeWarning: invalid value encountered in matmul\n","  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n","C:\\Users\\User\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_linear_loss.py:294: RuntimeWarning: invalid value encountered in matmul\n","  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\n"]},{"name":"stdout","output_type":"stream","text":["R2 score for TweedieRegressor: -0.02770042399019685\n","R2 score for SVR: -0.08188728003000033\n","R2 score for KNeighborsRegressor: 0.7985829882407782\n","R2 score for RandomForestRegressor: 0.9347341902863672\n","R2 score for AdaBoostRegressor: 0.9533956667180011\n","R2 score for GradientBoostingRegressor: 0.9396031635143189\n","R2 score for HistGradientBoostingRegressor: 0.8378476735827719\n","R2 score for XGBRegressor: 0.9168885927109579\n","\n","Best model: AdaBoostRegressor with R2 score: 0.9533956667180011\n"]}],"source":["# Question 17: Call the function you just created using the last version of the pipeline you created with this new function inserted AFTER you select the features. It will replace the second time you called fit_cv_regression in the pipeline. Use this function in the full version of the pipeline which should look something like this pseudocode:\n","\n","# 1. import the entire dataset\n","# 2. generate reach and drop the features specified in that function\n","# 3. call the date function to convert 'created_at_tweet' into three new columns and drop the original column\n","# 4. call the binning function to bin all remaining categorical features based on the cutoff that gave you the best model fit\n","# 5. run fit_cv_regression based on 5 folds with 5 repeats using the last version of the dataset\n","# 6. call the feature selection function to reduce the number of features; allow the function to print out the before-and-after feature counts\n","# 7. run fit_cv_regression_expanded based on 5 folds with 5 repeats using the last version of the dataset after feature selection\n","# don't worry about saving/deploying the model for this question\n","\n","# Step 1: Import the entire dataset\n","df = importData('tw_tweets_users_media.csv')\n","\n","# Step 2: Process the dataset to generate 'reach' and drop specified features\n","processed_df = process_dataset(df)\n","\n","# Step 3: Convert 'created_at_tweet' into new date-time related columns\n","processed_df = convert_date_column(processed_df, 'created_at_tweet')\n","\n","# Step 4: Create binned versions of the dataset based on the cutoff that gave the best model fit\n","df_best_binned = bin_groups(processed_df.copy(), cutoff=0.05, messages=False)\n","\n","# Step 5: Fit models using the revised fit_cv_regression function\n","best_model = fit_cv_regression(df_best_binned, label='reach', k=5, repeat=True, messages=False)\n","\n","# Step 6: Perform feature selection to reduce the number of features\n","df_selected = select_features(df_best_binned, label='reach', model=best_model)\n","\n","# Step 7: Fit models again after feature selection\n","best_model_after_selection = test_regression_algorithms(X=df_selected.drop(columns='reach'), y=df_selected['reach'], cv='rkf', messages=True)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyP1FGIpzLmpgTcFgrHgXEEd","mount_file_id":"1uAx8cgAVzoHdbakwfo8jx2_Je8Vvy-E2","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
